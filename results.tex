\subsection{Test Configuration}

Our benchmark was tested on a 72-core (4 sockets with 18 cores each capable of running 2 hardware threads, totalling 144 hardware threads) Intel Xeon machine clocked at 2.50 GHz with 512 GB of RAM and 4 memory banks. The machine is running Ubuntu 14.04 with kernel version 3.13.0-141. The with DEF code was compiled with DEF version 0.18.0a and the C code was compiled with Clang 6.0, and all code was compiled with -03 level of optimization.

During the benchmark threads were pinned programmatically to individual cores initially avoiding HyperThreading and later exhausting all individual execution units on a single CPU with the use of HyperThreading, before migrating to another CPU socket. JEMalloc\cite{JEMalloc} was used in all tests, as it had been in the Forkscan paper.\cite{Forkscan} The \texttt{numactl} Linux program was used to control which memory bank allocation was allowed to take place. The memory banks closest to the running CPUs were selected as they became active.

The microbenchmark measures the number of operations carried out over a specified amount of time rather than the time taken to execute a specified number of iterations. The rationale for this is threads finish their iterations before other threads. The remaining threads complete their iterations with less contention in the system, skewing the overall benchmark. Our benchmark reports the number of operations per second. The primary comparison is between the leaky C and the Forkscan enabled DEF implementations. Each benchmark is run for a total of 20 seconds each, with each configuration being sampled 5 times. The average of the runs is plotted.

Lastly, in figure \ref{fig:priorityqueues}, we tested the Lind{\'e}n Jonsson Queue and the SprayList priority queue.  The leaky implementations took a performance hit crossing the NUMA node boundary at 36 cores and never recovered.  One of the difficulties with a skip list-based priority queue is that many threads have a few early nodes in common in their caches.  As thread-counts increase, so do the number of invalidations, leading to the shape of the graph.  This is a worst-case data structure for Forkscan because priority queues are entirely updates, and the nodes are big (180 bytes, in this implementation).  However, again, the retiring DEF implementation has the same shape as the leaky ones.  It scales while they scale, and it's ultimately defeated by the same hurdle. The Lind{\'e}n Jonsson Queue beats the Spray List in our implementation. We found the Spray List's performance to be very sensitive to the parameter choice which scaled differently on for each thread count. One issue is that nodes were physically removed from the Spray List as soon as they were logically marked as deleted, increasing contention and degrading performance. 

In all of these structures, reclaiming memory scales up to the physical limits of the hardware, albeit with a gentler slope.  We attribute this to the on-demand aspect of memory tracking, versus at-allocation tracking; the memory that gets retired can mostly be de-allocated. Very little has to be recursively searched.

